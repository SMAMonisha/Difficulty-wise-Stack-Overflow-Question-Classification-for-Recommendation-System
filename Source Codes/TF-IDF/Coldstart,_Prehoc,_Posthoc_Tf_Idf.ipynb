{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWUrKNz69XvY"
      },
      "source": [
        "#Header"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT06PRayarHh",
        "outputId": "d5c05f66-799e-4698-fe99-6d8276cbfd3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ffKNxwvgxBz",
        "outputId": "f3fe7a50-b55c-4a06-9bc0-20a044222c98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-KcnhnAg2h8"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth \n",
        "auth.authenticate_user() \n",
        " \n",
        "import gspread \n",
        "from google.auth import default \n",
        "creds, _ = default() \n",
        " \n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urqe-WEkCu9C"
      },
      "source": [
        "#Metrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJe7-Fv3Y6Iz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def permanceMetrics(Y, pred, pred_prob):\n",
        "    acc = accuracy_score(Y,pred)\n",
        "    pre = precision_score(Y,pred,average='weighted')\n",
        "    re = recall_score(Y,pred, average='weighted')\n",
        "    f1 = f1_score(Y,pred, average='weighted')\n",
        "    acrc = roc_auc_score(Y,pred_prob,multi_class= 'ovr')\n",
        "    return acc, pre, re, f1, acrc\n",
        "\n",
        "def avgMetric(met):\n",
        "    res = np.array(met)\n",
        "    acc = res[::5].mean()\n",
        "    pre = res[1::5].mean()\n",
        "    re = res[2::5].mean()\n",
        "    f1 = res[3::5].mean()\n",
        "    acrc = res[4::5].mean()\n",
        "    return np.array([acc, pre, re, f1, acrc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RBsSZX49oaA"
      },
      "source": [
        "#Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T79stl1aAl98"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def step03(data):\n",
        "    step03_data = []\n",
        "    for question in data:\n",
        "        codeless_question=re.sub('<code>[^>]*</code>', '', question)\n",
        "        tagless_question=re.sub('<[^>]*>', '', codeless_question)\n",
        "        step03_data.append(tagless_question)\n",
        "    return step03_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB0gxbobxwPI"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(data):\n",
        "  step03_data=step03(data)\n",
        "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "  punctuationless_data = []\n",
        "  for question in step03_data:\n",
        "      no_punct = \"\"\n",
        "      for char in question:\n",
        "          if char not in punctuations:\n",
        "              no_punct = no_punct + char\n",
        "      punctuationless_data.append(no_punct)\n",
        "  return punctuationless_data\n",
        "\n",
        "# print(punctuationless_data[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajNQ41knohN9"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "\n",
        "def step01(data):\n",
        "  snow_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "  punctuationless_data=remove_punctuation(data)\n",
        "  step01_data = []\n",
        "  for question in punctuationless_data:\n",
        "    tokenized_question = []\n",
        "    tokenized_question = word_tokenize(question)\n",
        "    stem_question = []\n",
        "    for t_ques in tokenized_question:\n",
        "      stem_word = snow_stemmer.stem(t_ques)\n",
        "      stem_question.append(stem_word)\n",
        "    step01_data.append(stem_question)\n",
        "  return step01_data\n",
        "\n",
        "# print(step01_data[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn8cff6a48J6"
      },
      "outputs": [],
      "source": [
        "f=open('/content/drive/MyDrive/so papers/papers on stack overflow/User Expertise and Question Difficulty/Implementation/Stopwords.txt','r')\n",
        "stop_words=list(f.read().split(\"\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgdKeSpL6vOt"
      },
      "outputs": [],
      "source": [
        "def preprocessing(data):\n",
        "  stop_words2 = nltk.corpus.stopwords.words('english')\n",
        "  final_stopword_list = stop_words + stop_words2\n",
        "\n",
        "  step01_data=step01(data)\n",
        "  step02_data = []\n",
        "  for question in step01_data:\n",
        "    filtered_sentence=[]\n",
        "    for word in question:\n",
        "      if word not in final_stopword_list:\n",
        "          filtered_sentence.append(word)\n",
        "    step02_data.append(filtered_sentence)\n",
        "  return step02_data \n",
        "\n",
        "# print(step02_data[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YACAF79g96Ur"
      },
      "source": [
        "#Tf-Idf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrJd1tkMdc3v"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def tf_idf(preprocessed):\n",
        "    term_count=[]\n",
        "    tf=[]\n",
        "    #tf calculation\n",
        "    for word in preprocessed:\n",
        "        counts = Counter(word)\n",
        "        count_list = [(k, v) for k, v in counts.items()]\n",
        "        term_count.append(count_list)\n",
        "    for doc in term_count:\n",
        "        total_word=0\n",
        "        for word in doc:\n",
        "            total_word=total_word+word[1]\n",
        "        tf_list=[]\n",
        "        for word in doc:\n",
        "            temp=word[1]/ float(total_word)\n",
        "            tf_list.append((word[0],temp))\n",
        "        tf.append(tf_list)\n",
        "    #df calculation\n",
        "    df=dict()\n",
        "    for count_list in term_count:\n",
        "        for word in count_list:\n",
        "            if word[0] in df:\n",
        "                df[word[0]]=df[word[0]]+word[1]\n",
        "            else:\n",
        "                df[word[0]]=word[1]\n",
        "    deleting_key=[]\n",
        "    for k in df.keys():\n",
        "        if df[k]<3:\n",
        "            deleting_key.append(k)\n",
        "    for k in deleting_key:\n",
        "        del df[k]\n",
        "\n",
        "    #idf calculation\n",
        "    idf = dict()\n",
        "    for word in df:\n",
        "        idf[word] = math.log(len(term_count) / float(df[word]))\n",
        "    #tf-idf calculation\n",
        "    tfidf_list=[]\n",
        "    for count_list in tf:\n",
        "        document=[]\n",
        "        for word_list in count_list:\n",
        "            word=word_list[0]\n",
        "            if word_list[0] in idf:\n",
        "                tfidf=word_list[1]*idf[word_list[0]]\n",
        "            else:\n",
        "                tfidf=word_list[1]*0\n",
        "            tuple=(word,tfidf)\n",
        "            document.append(tuple)\n",
        "        tfidf_list.append(document)\n",
        "    return tfidf_list,df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7bZHls7bThg"
      },
      "outputs": [],
      "source": [
        "def vectorizer_tfidf(processed_data):\n",
        "    tfidf_list,df=tf_idf(processed_data)\n",
        "    features=[x for x in df.keys()]\n",
        "    tfidf_vector=[]\n",
        "    for doc in tfidf_list:\n",
        "        doc_wise_tfidf=[]\n",
        "        for word in features:\n",
        "            for term in doc:\n",
        "                value=0\n",
        "                if word==term[0]:\n",
        "                    value=term[1]\n",
        "                    break\n",
        "            doc_wise_tfidf.append(value)\n",
        "        tfidf_vector.append(doc_wise_tfidf)\n",
        "    tfidf_vector_df=pd.DataFrame([[y for y in  x] for x in tfidf_vector],columns=features)\n",
        "    return tfidf_vector_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH4K534j9jCZ"
      },
      "source": [
        "#Data Read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNi8F2N2g36b",
        "outputId": "ef106815-ab60-4c2c-8149-b028943a492b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1245\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "worksheet1 = gc.open('Final Dataset').get_worksheet(0)\n",
        "worksheet2 = gc.open('Final Dataset').get_worksheet(1)\n",
        "\n",
        "rows1 = worksheet1.get_all_values()\n",
        "rows2 = worksheet2.get_all_values()\n",
        "\n",
        "stackData1=pd.DataFrame.from_records(rows1,columns=rows1[0])\n",
        "stackData2=pd.DataFrame.from_records(rows2,columns=rows2[0])\n",
        "\n",
        "stackData1.drop(0, inplace=True, axis=0)\n",
        "stackData2.drop(0, inplace=True, axis=0)\n",
        "\n",
        "stackData=[stackData1, stackData2]\n",
        "stackData=pd.concat(stackData,ignore_index=True)\n",
        "\n",
        "stackData['ProcessedBody']=stackData['ProcessedBody']+stackData['Tags']+stackData['Title']\n",
        "\n",
        "\n",
        "feature=[\"ProcessedBody\", \"LOC\", \"QuestionLength\",\t\"Url+ImageCount\",\t\"Reputation\",\t\"user_badge_bronze_counts\",\t\"user_badge_gold_counts\",\t\"user_badge_silver_counts\",\t\"accept_rate\" , \"view_count\",\t\"answer_count\",\t\"favorite_count\",\t\"question_score\",\t\"up_vote_count\",\t\"First_answer_Interval\",\"Accept_answer_Interval\"]\n",
        "#---------count------------#\n",
        "stackData[\"view_count\"] = pd.to_numeric(stackData[\"view_count\"])\n",
        "stackData[\"answer_count\"] = pd.to_numeric(stackData[\"answer_count\"])\n",
        "stackData[\"favorite_count\"] = pd.to_numeric(stackData[\"favorite_count\"])\n",
        "stackData[\"question_score\"] = pd.to_numeric(stackData[\"question_score\"])\n",
        "stackData[\"up_vote_count\"] = pd.to_numeric(stackData[\"up_vote_count\"])\n",
        "#---------Data------------,\t\"First_answer_date\",\t\"Accepted_answer_date\"#\n",
        "stackData[\"creation_date\"] = pd.to_datetime(stackData[\"creation_date\"],unit='s')\n",
        "stackData[\"First_answer_date\"] = pd.to_datetime(stackData[\"First_answer_date\"],unit='s')\n",
        "stackData[\"Accepted_answer_date\"] = pd.to_datetime(stackData[\"Accepted_answer_date\"],unit='s')\n",
        "\n",
        "# print(stackData[\"creation_date\"],stackData[\"First_answer_date\"],stackData[\"Accepted_answer_date\"])\n",
        "#----------Date Interval ------#\n",
        "stackData[\"First_answer_Interval\"]=(stackData[\"First_answer_date\"]-stackData[\"creation_date\"])/pd.Timedelta(minutes=1)\n",
        "stackData[\"Accept_answer_Interval\"]=(stackData[\"Accepted_answer_date\"]-stackData[\"creation_date\"])/pd.Timedelta(minutes=1)\n",
        "\n",
        "stackData[\"First_answer_Interval\"] = stackData[\"First_answer_Interval\"].apply(lambda x: -1 if x <= 0 else x)\n",
        "stackData[\"Accept_answer_Interval\"]=stackData[\"Accept_answer_Interval\"].apply(lambda x: -1 if x <= 0 else x)\n",
        "# print(stackData[feature])\n",
        "data=stackData[feature].values\n",
        "y=stackData.Label.values\n",
        "print(len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLpoTW-3EWPC"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NedtK2yQIR2u"
      },
      "outputs": [],
      "source": [
        "processed_data=preprocessing(data[:,0])\n",
        "tfidf_vector_df=vectorizer_tfidf(processed_data)\n",
        "print(tfidf_vector_df.shape)\n",
        "\n",
        "#for cold start\n",
        "# other_features=data[:,1:4]\n",
        "# other_features_df=pd.DataFrame([[x for x in col] for col in other_features],columns=feature[1:])\n",
        "# print(other_features_df.shape)\n",
        "\n",
        "# for pre hoc\n",
        "# other_features=data[:,1:9]\n",
        "# other_features_df=pd.DataFrame([[x for x in col] for col in other_features],columns=feature[1:])\n",
        "# print(other_features_df.shape)\n",
        "\n",
        "# for post hoc\n",
        "other_features=data[:,1:]\n",
        "other_features_df=pd.DataFrame([[x for x in col] for col in other_features],columns=feature[1:])\n",
        "print(other_features_df.shape)\n",
        "\n",
        "all_features_df=pd.concat([tfidf_vector_df,other_features_df], axis=1)\n",
        "print(all_features_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_9HTxHWFeTh"
      },
      "source": [
        "#Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ2yFtUZYahT",
        "outputId": "e5e70240-3245-4276-f6cb-82b5d5f0eecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics :  Accuracy \t\tPrecision \t\tRecall \t\tF1-score \tAUC_ROC\n",
            "Fold- 1 :  0.608 0.5260495867768595 0.608 0.47846837606837606 0.6487961327662531\n",
            "[[ 0  8  2]\n",
            " [ 0 74  0]\n",
            " [ 0 39  2]]\n",
            "Fold- 2 :  0.616 0.4968242424242424 0.616 0.5218757763975156 0.6949570416188219\n",
            "[[ 0  2  8]\n",
            " [ 0 72  2]\n",
            " [ 0 36  5]]\n",
            "Fold- 3 :  0.624 0.5120109890109891 0.624 0.5254289127837516 0.7337243253037652\n",
            "[[ 0  3  7]\n",
            " [ 0 73  1]\n",
            " [ 0 36  5]]\n",
            "Fold- 4 :  0.608 0.5998868686868688 0.608 0.5587948852688737 0.7539536413000326\n",
            "[[ 1  3  6]\n",
            " [ 0 65  9]\n",
            " [ 0 31 10]]\n",
            "Fold- 5 :  0.656 0.5927070707070706 0.656 0.6204444444444445 0.766987382823296\n",
            "[[ 0  5  6]\n",
            " [ 0 63 11]\n",
            " [ 1 20 19]]\n",
            "Fold- 6 :  0.6451612903225806 0.5778125396355641 0.6451612903225806 0.5619799555283427 0.8227283085047868\n",
            "[[ 0  5  5]\n",
            " [ 0 72  1]\n",
            " [ 1 32  8]]\n",
            "Fold- 7 :  0.6854838709677419 0.6714905870480565 0.6854838709677419 0.6330966566238572 0.8868149021332129\n",
            "[[ 1  5  4]\n",
            " [ 1 71  1]\n",
            " [ 1 27 13]]\n",
            "Fold- 8 :  0.6451612903225806 0.5782925054571914 0.6451612903225806 0.5776519270267004 0.7456040795939168\n",
            "[[ 0  6  4]\n",
            " [ 0 69  4]\n",
            " [ 0 30 11]]\n",
            "Fold- 9 :  0.5967741935483871 0.5629821178120618 0.5967741935483871 0.4935507428153766 0.7170658718041114\n",
            "[[ 1  7  2]\n",
            " [ 0 70  3]\n",
            " [ 0 38  3]]\n",
            "Fold- 10 :  0.6048387096774194 0.5756020616724429 0.6048387096774194 0.47310820207394866 0.6371515974071974\n",
            "[[ 0  9  1]\n",
            " [ 0 73  0]\n",
            " [ 0 39  2]]\n",
            "\n",
            "Average:  0.6289419354838709 0.5693658569231348 0.6289419354838709 0.5444399879031186 0.7407783283255395\n"
          ]
        }
      ],
      "source": [
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "# warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
        "# from sklearn import model_selection\n",
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.model_selection import StratifiedKFold\n",
        "# from sklearn.ensemble import RandomForestClassifier as RF\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# skf = StratifiedKFold(n_splits=10)\n",
        "\n",
        "# i=0\n",
        "\n",
        "# all_features_df=np.array(all_features_df)\n",
        "\n",
        "\n",
        "# rm = []\n",
        "# print('Metrics :  Accuracy \\t\\tPrecision \\t\\tRecall \\t\\tF1-score \\tAUC_ROC')\n",
        "# for train_index, test_index in skf.split(all_features_df,y):\n",
        "#     i=i+1\n",
        "#     X_train, X_test = all_features_df[train_index], all_features_df[test_index]\n",
        "#     y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "#     rf = RF(n_estimators = 15, max_depth = 8, criterion='entropy', random_state = 42)\n",
        "#     rf.fit(X_train,y_train)\n",
        "#     pred = rf.predict(X_test)\n",
        "#     pred_prob = rf.predict_proba(X_test)\n",
        "#     acc, pre, re, f1, arc = permanceMetrics(y_test, pred, pred_prob)\n",
        "#     print('Fold-',i,': ', acc, pre, re, f1,arc)\n",
        "#     rm += [acc, pre, re, f1,arc]\n",
        "#     cm = confusion_matrix(y_test, pred)\n",
        "#     print(cm)\n",
        "# acc, pre, re, f1, acrc = avgMetric(rm)\n",
        "# print('\\nAverage: ', acc, pre, re, f1, acrc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQpsFYs2Fit4"
      },
      "source": [
        "#XG Boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eQHp-ScFoAf",
        "outputId": "a3ab8f5b-3ddb-4673-8279-a007c6f15525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics :  Accuracy \t\tPrecision \t\tRecall \t\tF1-score \tAUC_ROC\n",
            "Fold- 1 :  0.672 0.6430914927768859 0.672 0.6476796775436412 0.7454489514510723\n",
            "[[ 1  3  6]\n",
            " [ 1 67  6]\n",
            " [ 6 19 16]]\n",
            "Fold- 2 :  0.664 0.6377904761904762 0.664 0.6371123711968784 0.7785891540417147\n",
            "[[ 2  3  5]\n",
            " [ 0 65  9]\n",
            " [ 2 23 16]]\n",
            "Fold- 3 :  0.688 0.6969290322580645 0.688 0.6609046695341712 0.7791702386150643\n",
            "[[ 2  4  4]\n",
            " [ 0 66  8]\n",
            " [ 0 23 18]]\n",
            "Fold- 4 :  0.616 0.5575241502683363 0.616 0.5842 0.7202983347285011\n",
            "[[ 0  4  6]\n",
            " [ 0 59 15]\n",
            " [ 0 23 18]]\n",
            "Fold- 5 :  0.656 0.6411594202898552 0.656 0.6279104991394148 0.7252344252499051\n",
            "[[ 2  4  5]\n",
            " [ 1 64  9]\n",
            " [ 0 24 16]]\n",
            "Fold- 6 :  0.6451612903225806 0.6196998447472831 0.6451612903225806 0.6266526326244579 0.7283859869408221\n",
            "[[ 2  3  5]\n",
            " [ 1 61 11]\n",
            " [ 3 21 17]]\n",
            "Fold- 7 :  0.6774193548387096 0.6380408586029566 0.6774193548387096 0.641662638436832 0.7859111346955533\n",
            "[[ 1  1  8]\n",
            " [ 1 67  5]\n",
            " [ 1 24 16]]\n",
            "Fold- 8 :  0.6854838709677419 0.6702812980030722 0.6854838709677419 0.6661186174171703 0.7625899608456524\n",
            "[[ 1  2  7]\n",
            " [ 0 61 12]\n",
            " [ 1 17 23]]\n",
            "Fold- 9 :  0.6774193548387096 0.6371009876293524 0.6774193548387096 0.649124268277494 0.799091665946564\n",
            "[[ 1  0  9]\n",
            " [ 0 66  7]\n",
            " [ 3 21 17]]\n",
            "Fold- 10 :  0.6451612903225806 0.6318077084206116 0.6451612903225806 0.6375257104036358 0.7912360366891423\n",
            "[[ 2  1  7]\n",
            " [ 2 58 13]\n",
            " [ 3 18 20]]\n",
            "\n",
            "Average:  0.6626645161290322 0.6373425269186894 0.6626645161290322 0.6378891084573695 0.7615955889203991\n"
          ]
        }
      ],
      "source": [
        "# from xgboost import XGBClassifier\n",
        "# from sklearn import model_selection\n",
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.model_selection import StratifiedKFold\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# # import scikitplot as skplt\n",
        "# import matplotlib.pyplot as plt\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "# warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
        "\n",
        "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# i=0\n",
        "\n",
        "# all_features_df=np.array(all_features_df)\n",
        "\n",
        "\n",
        "# rm = []\n",
        "# print('Metrics :  Accuracy \\t\\tPrecision \\t\\tRecall \\t\\tF1-score \\tAUC_ROC')\n",
        "# for train_index, test_index in kfold.split(all_features_df,y):\n",
        "#     i=i+1\n",
        "#     X_train, X_test = all_features_df[train_index], all_features_df[test_index]\n",
        "#     y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "#     my_model = XGBClassifier(n_estimators=40,learning_rate=0.05, max_depth=8)\n",
        "#     my_model.fit(X_train, y_train)\n",
        "#     pred = my_model.predict(X_test)\n",
        "#     pred_prob = my_model.predict_proba(X_test)\n",
        "\n",
        "\n",
        "#     acc, pre, re, f1, arc = permanceMetrics(y_test, pred, pred_prob)\n",
        "#     print('Fold-',i,': ', acc, pre, re, f1,arc)\n",
        "#     rm += [acc, pre, re, f1,arc]\n",
        "#     cm = confusion_matrix(y_test, pred)\n",
        "#     print(cm)\n",
        "# acc, pre, re, f1, acrc = avgMetric(rm)\n",
        "# print('\\nAverage: ', acc, pre, re, f1, acrc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDeO_AhQGTIU"
      },
      "source": [
        "#ADA Boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nISP-Ch4GYdq"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# import scikitplot as skplt\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
        "\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "i=0\n",
        "\n",
        "all_features_df=np.array(all_features_df)\n",
        "\n",
        "\n",
        "rm = []\n",
        "print('Metrics :  Accuracy \\t\\tPrecision \\t\\tRecall \\t\\tF1-score \\tAUC_ROC')\n",
        "for train_index, test_index in kfold.split(all_features_df,y):\n",
        "    i=i+1\n",
        "    X_train, X_test = all_features_df[train_index], all_features_df[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    my_model = AdaBoostClassifier(n_estimators=1000,learning_rate=0.05)\n",
        "    my_model.fit(X_train, y_train)\n",
        "    pred = my_model.predict(X_test)\n",
        "    pred_prob = my_model.predict_proba(X_test)\n",
        "\n",
        "    \n",
        "    acc, pre, re, f1, arc = permanceMetrics(y_test, pred, pred_prob)\n",
        "    print('Fold-',i,': ', acc, pre, re, f1,arc)\n",
        "    rm += [acc, pre, re, f1,arc]\n",
        "    cm = confusion_matrix(y_test, pred)\n",
        "    print(cm)\n",
        "acc, pre, re, f1, acrc = avgMetric(rm)\n",
        "print('\\nAverage: ', acc, pre, re, f1, acrc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVM"
      ],
      "metadata": {
        "id": "L1w7aXDsuYz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# import scikitplot as skplt\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
        "\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "i=0\n",
        "\n",
        "all_features_df=np.array(all_features_df)\n",
        "\n",
        "\n",
        "rm = []\n",
        "print('Metrics :  Accuracy \\t\\tPrecision \\t\\tRecall \\t\\tF1-score \\tAUC_ROC')\n",
        "for train_index, test_index in kfold.split(all_features_df,y):\n",
        "    i=i+1\n",
        "    X_train, X_test = all_features_df[train_index], all_features_df[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    my_model = svm.SVC(decision_function_shape='ovr',probability=True)\n",
        "    my_model.fit(X_train, y_train)\n",
        "    pred = my_model.predict(X_test)\n",
        "    pred_prob = my_model.predict_proba(X_test)\n",
        "\n",
        "    \n",
        "    acc, pre, re, f1, arc = permanceMetrics(y_test, pred, pred_prob)\n",
        "    print('Fold-',i,': ', acc, pre, re, f1,arc)\n",
        "    rm += [acc, pre, re, f1,arc]\n",
        "    cm = confusion_matrix(y_test, pred)\n",
        "    print(cm)\n",
        "acc, pre, re, f1, acrc = avgMetric(rm)\n",
        "print('\\nAverage: ', acc, pre, re, f1, acrc)"
      ],
      "metadata": {
        "id": "h9mrSIq9ubx7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gWUrKNz69XvY",
        "Urqe-WEkCu9C",
        "7RBsSZX49oaA",
        "YACAF79g96Ur",
        "y_9HTxHWFeTh",
        "PQpsFYs2Fit4"
      ],
      "name": "Coldstart, Prehoc, Posthoc Tf_Idf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}