{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Header"
      ],
      "metadata": {
        "id": "gWUrKNz69XvY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT06PRayarHh",
        "outputId": "dc58da9f-bc34-407f-b999-de9f62005875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ffKNxwvgxBz",
        "outputId": "d2f293d7-0cb2-4f8c-a7db-d3e2c3fc2ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-KcnhnAg2h8"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth \n",
        "auth.authenticate_user() \n",
        " \n",
        "import gspread \n",
        "from google.auth import default \n",
        "creds, _ = default() \n",
        " \n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metrices"
      ],
      "metadata": {
        "id": "Urqe-WEkCu9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def permanceMetrics(Y, pred, pred_prob):\n",
        "    acc = accuracy_score(Y,pred)\n",
        "    pre = precision_score(Y,pred,average='weighted')\n",
        "    re = recall_score(Y,pred, average='weighted')\n",
        "    f1 = f1_score(Y,pred, average='weighted')\n",
        "    acrc = roc_auc_score(Y,pred_prob,multi_class= 'ovr')\n",
        "    return acc, pre, re, f1, acrc\n",
        "\n",
        "def avgMetric(met):\n",
        "    res = np.array(met)\n",
        "    acc = res[::5].mean()\n",
        "    pre = res[1::5].mean()\n",
        "    re = res[2::5].mean()\n",
        "    f1 = res[3::5].mean()\n",
        "    acrc = res[4::5].mean()\n",
        "    return np.array([acc, pre, re, f1, acrc])"
      ],
      "metadata": {
        "id": "tJe7-Fv3Y6Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess"
      ],
      "metadata": {
        "id": "7RBsSZX49oaA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T79stl1aAl98"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def step03(data):\n",
        "    step03_data = []\n",
        "    for question in data:\n",
        "        codeless_question=re.sub('<code>[^>]*</code>', '', question)\n",
        "        tagless_question=re.sub('<[^>]*>', '', codeless_question)\n",
        "        step03_data.append(tagless_question)\n",
        "    return step03_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB0gxbobxwPI"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(data):\n",
        "  step03_data=step03(data)\n",
        "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "  punctuationless_data = []\n",
        "  for question in step03_data:\n",
        "      no_punct = \"\"\n",
        "      for char in question:\n",
        "          if char not in punctuations:\n",
        "              no_punct = no_punct + char\n",
        "      punctuationless_data.append(no_punct)\n",
        "  return punctuationless_data\n",
        "\n",
        "# print(punctuationless_data[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajNQ41knohN9"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "\n",
        "def step01(data):\n",
        "  snow_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "  punctuationless_data=remove_punctuation(data)\n",
        "  step01_data = []\n",
        "  for question in punctuationless_data:\n",
        "    tokenized_question = []\n",
        "    tokenized_question = word_tokenize(question)\n",
        "    stem_question = []\n",
        "    for t_ques in tokenized_question:\n",
        "      stem_word = snow_stemmer.stem(t_ques)\n",
        "      stem_question.append(stem_word)\n",
        "    step01_data.append(stem_question)\n",
        "  return step01_data\n",
        "\n",
        "# print(step01_data[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn8cff6a48J6"
      },
      "outputs": [],
      "source": [
        "f=open('/content/drive/MyDrive/papers on stack overflow/User Expertise and Question Difficulty/Implementation/Stopwords.txt','r')\n",
        "stop_words=list(f.read().split(\"\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgdKeSpL6vOt"
      },
      "outputs": [],
      "source": [
        "def preprocessing(data):\n",
        "  stop_words2 = nltk.corpus.stopwords.words('english')\n",
        "  final_stopword_list = stop_words + stop_words2\n",
        "\n",
        "  step01_data=step01(data)\n",
        "  step02_data = []\n",
        "  for question in step01_data:\n",
        "    filtered_sentence=[]\n",
        "    for word in question:\n",
        "      if word not in final_stopword_list:\n",
        "          filtered_sentence.append(word)\n",
        "    step02_data.append(filtered_sentence)\n",
        "  return step02_data \n",
        "\n",
        "# print(step02_data[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tf-Idf\n"
      ],
      "metadata": {
        "id": "YACAF79g96Ur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrJd1tkMdc3v"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def tf_idf(preprocessed):\n",
        "    term_count=[]\n",
        "    tf=[]\n",
        "    #tf calculation\n",
        "    for word in preprocessed:\n",
        "        counts = Counter(word)\n",
        "        count_list = [(k, v) for k, v in counts.items()]\n",
        "        term_count.append(count_list)\n",
        "    for doc in term_count:\n",
        "        total_word=0\n",
        "        for word in doc:\n",
        "            total_word=total_word+word[1]\n",
        "        tf_list=[]\n",
        "        for word in doc:\n",
        "            temp=word[1]/ float(total_word)\n",
        "            tf_list.append((word[0],temp))\n",
        "        tf.append(tf_list)\n",
        "    #df calculation\n",
        "    df=dict()\n",
        "    for count_list in term_count:\n",
        "        for word in count_list:\n",
        "            if word[0] in df:\n",
        "                df[word[0]]=df[word[0]]+word[1]\n",
        "            else:\n",
        "                df[word[0]]=word[1]\n",
        "    deleting_key=[]\n",
        "    for k in df.keys():\n",
        "        if df[k]<3:\n",
        "            deleting_key.append(k)\n",
        "    for k in deleting_key:\n",
        "        del df[k]\n",
        "\n",
        "    #idf calculation\n",
        "    idf = dict()\n",
        "    for word in df:\n",
        "        idf[word] = math.log(len(term_count) / float(df[word]))\n",
        "    #tf-idf calculation\n",
        "    tfidf_list=[]\n",
        "    for count_list in tf:\n",
        "        document=[]\n",
        "        for word_list in count_list:\n",
        "            word=word_list[0]\n",
        "            if word_list[0] in idf:\n",
        "                tfidf=word_list[1]*idf[word_list[0]]\n",
        "            else:\n",
        "                tfidf=word_list[1]*0\n",
        "            tuple=(word,tfidf)\n",
        "            document.append(tuple)\n",
        "        tfidf_list.append(document)\n",
        "    return tfidf_list,df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer_tfidf(processed_data):\n",
        "    tfidf_list,df=tf_idf(processed_data)\n",
        "    features=[x for x in df.keys()]\n",
        "    tfidf_vector=[]\n",
        "    for doc in tfidf_list:\n",
        "        doc_wise_tfidf=[]\n",
        "        for word in features:\n",
        "            for term in doc:\n",
        "                value=0\n",
        "                if word==term[0]:\n",
        "                    value=term[1]\n",
        "                    break\n",
        "            doc_wise_tfidf.append(value)\n",
        "        tfidf_vector.append(doc_wise_tfidf)\n",
        "    tfidf_vector_df=pd.DataFrame([[y for y in  x] for x in tfidf_vector],columns=features)\n",
        "    return tfidf_vector_df"
      ],
      "metadata": {
        "id": "l7bZHls7bThg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Read"
      ],
      "metadata": {
        "id": "kH4K534j9jCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNi8F2N2g36b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "worksheet1 = gc.open('Final Dataset').get_worksheet(0)\n",
        "worksheet2 = gc.open('Final Dataset').get_worksheet(1)\n",
        "\n",
        "rows1 = worksheet1.get_all_values()\n",
        "rows2 = worksheet2.get_all_values()\n",
        "\n",
        "stackData1=pd.DataFrame.from_records(rows1,columns=rows1[0])\n",
        "stackData2=pd.DataFrame.from_records(rows2,columns=rows2[0])\n",
        "\n",
        "stackData1.drop(0, inplace=True, axis=0)\n",
        "stackData2.drop(0, inplace=True, axis=0)\n",
        "\n",
        "stackData=[stackData1, stackData2]\n",
        "stackData=pd.concat(stackData,ignore_index=True)\n",
        "\n",
        "feature=[\"Body\",\"QuestionLength\", \"Reputation\",\"First_answer_Interval\",\"favorite_count\",\"question_score\",\"view_count\",\"answer_count\"]\n",
        "\n",
        "stackData[\"view_count\"] = pd.to_numeric(stackData[\"view_count\"])\n",
        "stackData[\"answer_count\"] = pd.to_numeric(stackData[\"answer_count\"])\n",
        "stackData[\"favorite_count\"] = pd.to_numeric(stackData[\"favorite_count\"])\n",
        "stackData[\"question_score\"] = pd.to_numeric(stackData[\"question_score\"])\n",
        "\n",
        "stackData[\"creation_date\"] = pd.to_datetime(stackData[\"creation_date\"],unit='s')\n",
        "stackData[\"First_answer_date\"] = pd.to_datetime(stackData[\"First_answer_date\"],unit='s')\n",
        "stackData[\"First_answer_Interval\"]=(stackData[\"First_answer_date\"]-stackData[\"creation_date\"])/pd.Timedelta(minutes=1)\n",
        "stackData[\"First_answer_Interval\"] = stackData[\"First_answer_Interval\"].apply(lambda x: -1 if x <= 0 else x)\n",
        "# stackData[\"Accept_answer_Interval\"]=stackData[\"Accept_answer_Interval\"].apply(lambda x: -1 if x <= 0 else x)\n",
        "\n",
        "data=stackData[feature].values\n",
        "y=stackData.Label.values\n",
        "print(len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "rLpoTW-3EWPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data=preprocessing(data[:,0])\n",
        "tfidf_vector_df=vectorizer_tfidf(processed_data)\n",
        "print(tfidf_vector_df.shape)\n",
        "\n",
        "other_features=data[:,1:8]\n",
        "other_features_df=pd.DataFrame([[x for x in col] for col in other_features],columns=feature[1:8])\n",
        "print(other_features_df.shape)\n",
        "\n",
        "all_features_df=pd.concat([tfidf_vector_df,other_features_df], axis=1)\n",
        "print(all_features_df.shape)"
      ],
      "metadata": {
        "id": "NedtK2yQIR2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier as RF\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "\n",
        "i=0\n",
        "\n",
        "all_features_df=np.array(all_features_df)\n",
        "\n",
        "\n",
        "rm = []\n",
        "print('Metrics :  Accuracy \\t\\tPrecision \\t\\tRecall \\t\\tF1-score \\tAUC_ROC')\n",
        "for train_index, test_index in skf.split(all_features_df,y):\n",
        "    i=i+1\n",
        "    X_train, X_test = all_features_df[train_index], all_features_df[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    rf = RF(n_estimators = 15, max_depth = 8, criterion='entropy', random_state = 42)\n",
        "    rf.fit(X_train,y_train)\n",
        "    pred = rf.predict(X_test)\n",
        "    pred_prob = rf.predict_proba(X_test)\n",
        "    acc, pre, re, f1, arc = permanceMetrics(y_test, pred, pred_prob)\n",
        "    print('Fold-',i,': ', acc, pre, re, f1,arc)\n",
        "    rm += [acc, pre, re, f1,arc]\n",
        "    cm = confusion_matrix(y_test, pred)\n",
        "    print(cm)\n",
        "acc, pre, re, f1, acrc = avgMetric(rm)\n",
        "print('\\nAverage: ', acc, pre, re, f1, acrc)"
      ],
      "metadata": {
        "id": "XZ2yFtUZYahT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PostHoc_soqde.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}